{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 영화리뷰 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "# 영화 한줄평 데이터 불러오기\n",
    "with open('train_data.json', encoding=\"utf-8\") as data_file:    \n",
    "    data = json.load(data_file)\n",
    "data = DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class 3개로 나누기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 리뷰 3개의 class로 나눈다 (\"POS\", \"NEU\", \"NEG\")\n",
    "data['rate'] = np.where(data['rating']>=8, 1,\n",
    "                       np.where(data['rating']>=4, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encoding\n",
    "import keras\n",
    "label = keras.utils.to_categorical(data[\"rate\"], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['date']\n",
    "del data['movie_id']\n",
    "del data['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>종합 평점은 4점 드립니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>원작이 칭송받는 이유는 웹툰 계 자체의 질적 저하가 심각하기 때문.  원작이나 영화...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>나름의  감동도 있고 안타까운 마음에 가슴도 먹먹  배우들의 연기가 good 김수현...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이런걸 돈주고 본 내자신이 후회스럽다 최악의 쓰레기 영화 김수현 밖에없는 저질 삼류영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>초반엔 코미디, 후반엔 액션, 결론은 코미디.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rate\n",
       "0                                    종합 평점은 4점 드립니다.     2\n",
       "1  원작이 칭송받는 이유는 웹툰 계 자체의 질적 저하가 심각하기 때문.  원작이나 영화...     0\n",
       "2  나름의  감동도 있고 안타까운 마음에 가슴도 먹먹  배우들의 연기가 good 김수현...     1\n",
       "3   이런걸 돈주고 본 내자신이 후회스럽다 최악의 쓰레기 영화 김수현 밖에없는 저질 삼류영화     0\n",
       "4                          초반엔 코미디, 후반엔 액션, 결론은 코미디.     2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train / test 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630000 70000\n",
      "630000 70000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split \n",
    "\n",
    "train, test, label_train, label_test = train_test_split(data['review'], label, random_state = 0, test_size=0.1)\n",
    "\n",
    "print(len(train), len(test))   \n",
    "print(len(label_train), len(label_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리(형태소분석)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'올해 최고의 영화'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 필요없는 숫자 등 제거\n",
    "text1 = [re.sub('\\d+',' ',tmp) for tmp in train]\n",
    "train_text = [re.sub('\\W+',' ',tmp) for tmp in text1]\n",
    "\n",
    "text1 = [re.sub('\\d+',' ',tmp) for tmp in test]\n",
    "test_text = [re.sub('\\W+',' ',tmp) for tmp in text1]\n",
    "\n",
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "\n",
    "train_tagger = [twitter.pos(line) for line in train_text]\n",
    "train_tagger[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tagger = [twitter.pos(line) for line in test_text]\n",
    "test_tagger[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pumsa(doc):    \n",
    "    token = []\n",
    "    rate = []\n",
    "    \n",
    "    for items in doc:        \n",
    "        words = []\n",
    "        for item in items: \n",
    "            if (item[1] in ['Noun', 'Verb', 'Adjective'])&(len(item[0])>1):\n",
    "                words.append(item[0])\n",
    "        token.append(words)\n",
    "    \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pumsa = pumsa(train_tagger)\n",
    "test_pumsa = pumsa(test_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(train_pumsa, size=100, window=10, min_count=10, workers=4, sg=1) # Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input embedding(mean, tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_embedding(word2vec, text):\n",
    "    return np.array([np.mean([word2vec[w] for w in words if w in word2vec] or [np.zeros(100)], axis=0) for words in text])\n",
    "\n",
    "train_m = mean_embedding(w2v, train_pumsa)\n",
    "test_m = mean_embedding(w2v, test_pumsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "# word2vec embedding vector와 word에 대한 tfidf 가중치를 이용한 vectorizer 함수\n",
    "class TfidfEmbeddingVectorizer:\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        \n",
    "    def transform(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer = lambda x : x) \n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_) \n",
    "        word2weight = defaultdict(lambda : max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]) \n",
    "        \n",
    "        array_list =[]\n",
    "        for words in X:\n",
    "            array_list.append(np.array(np.mean([self.word2vec[w]*word2weight[w] for w in words if w in self.word2vec] or [np.zeros(100)], axis = 0)))\n",
    "        return(array_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_tf = TfidfEmbeddingVectorizer(w2v)\n",
    "train_tf = vec_tf.transform(train_pumsa)\n",
    "test_tf = vec_tf.transform(test_pumsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = np.array(train_tf)\n",
    "test_tf = np.array(test_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, GRU, RNN, Flatten\n",
    "from keras.engine.topology import Input \n",
    "from keras.optimizers import Adagrad, SGD\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "model.add(Dense(64, activation='relu', input_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "ada = Adagrad(lr=0.01, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=ada,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "560000/560000 [==============================] - 8s 15us/step - loss: 0.7483 - acc: 0.6837\n",
      "Epoch 2/20\n",
      "560000/560000 [==============================] - 7s 13us/step - loss: 0.7176 - acc: 0.6979\n",
      "Epoch 3/20\n",
      "560000/560000 [==============================] - 7s 13us/step - loss: 0.7105 - acc: 0.7017\n",
      "Epoch 4/20\n",
      "560000/560000 [==============================] - 7s 13us/step - loss: 0.7057 - acc: 0.7034\n",
      "Epoch 5/20\n",
      "560000/560000 [==============================] - 7s 13us/step - loss: 0.7025 - acc: 0.7056\n",
      "Epoch 6/20\n",
      "560000/560000 [==============================] - 7s 12us/step - loss: 0.7006 - acc: 0.7067\n",
      "Epoch 7/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6986 - acc: 0.7073\n",
      "Epoch 8/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6973 - acc: 0.7080\n",
      "Epoch 9/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6954 - acc: 0.7090\n",
      "Epoch 10/20\n",
      "560000/560000 [==============================] - 8s 15us/step - loss: 0.6948 - acc: 0.7096\n",
      "Epoch 11/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6936 - acc: 0.7100\n",
      "Epoch 12/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6929 - acc: 0.7104\n",
      "Epoch 13/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6917 - acc: 0.7109\n",
      "Epoch 14/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6912 - acc: 0.7108\n",
      "Epoch 15/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6905 - acc: 0.7109\n",
      "Epoch 16/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6903 - acc: 0.7110\n",
      "Epoch 17/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6891 - acc: 0.7115\n",
      "Epoch 18/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6889 - acc: 0.7119\n",
      "Epoch 19/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6884 - acc: 0.7120\n",
      "Epoch 20/20\n",
      "560000/560000 [==============================] - 8s 14us/step - loss: 0.6882 - acc: 0.7117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1259c6ac8>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_tf, label_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000/140000 [==============================] - 1s 7us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_tf, label_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6700169602530344, 0.7175785714285714]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score #[loss, accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN을 위한 input embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_embedding(text):\n",
    "    return np.array([w2v[w] for w in text if w in w2v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb = list(map(seq_embedding,train_pumsa))\n",
    "test_emb = list(map(seq_embedding,test_pumsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 길이 맞춰주기(반복 패딩)\n",
    "train_pd = sequence.pad_sequences(train_emb, maxlen=10, dtype='float64')\n",
    "test_pd = sequence.pad_sequences(test_emb, maxlen=10, dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-581-22eba94cfd82>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-581-22eba94cfd82>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    model.compile(optimizer=ada, 'categorical_crossentropy', metrics=['accuracy'])\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#model.add(Embedding()) # input_dim / output_dim / input_length\n",
    "model.add(LSTM(128, input_shape=(10, 100)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "          \n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(optimizer=ada, 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(train_pd, label_train,\n",
    "          batch_size=128,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000/70000 [==============================] - 4s 61us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_pd, label_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7147932805470057, 0.7174142857006618]"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN (미완성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 10, 100, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reshape(x):\n",
    "    return np.reshape(x,(-1,10,100,1))\n",
    "\n",
    "train_re = reshape(train_pd)\n",
    "test_re = reshape(test_pd)\n",
    "train_re.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 1s 73us/step - loss: 0.9440 - acc: 0.6191\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.9131 - acc: 0.6281\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.9130 - acc: 0.6281\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.9135 - acc: 0.6281\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.9117 - acc: 0.6281\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.9116 - acc: 0.6281\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.9114 - acc: 0.6281\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9124 - acc: 0.6281\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.9117 - acc: 0.6281\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.9114 - acc: 0.6281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x110632ef0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (2, 100), activation='relu', input_shape=(10,100,1)))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#model.add(Conv2D(64, (2, 100), activation='relu'))\n",
    "#model.add(Conv2D(64, (2, 100), activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_re, label_train, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 60us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_re, label_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9214001455307007, 0.6249999990463256]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "text_clf = clf.fit(train_m, label_train)\n",
    "predicted_kkn = clf.predict(test_m)\n",
    "np.mean(predicted_knn == label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "text_clf = clf.fit(train_tf, label_train)\n",
    "predicted_kkn = clf.predict(test_tf)\n",
    "np.mean(predicted_knn == label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 100)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
